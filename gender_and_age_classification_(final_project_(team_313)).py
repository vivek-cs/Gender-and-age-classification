# -*- coding: utf-8 -*-
"""Gender and Age Classification (Final Project (Team 313)).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJzZC9lOspx0AMOfVxHWDW7fTVjJ5-kY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
from torchvision import get_image_backend

import torch
import torchvision
import torchsummary
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import torchvision.transforms as transforms
import torchvision.datasets as datasets

from google.colab import files

from tqdm.notebook import tqdm
import warnings
import os

warnings.filterwarnings('ignore')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

data  = pd.read_csv('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/fold_0_data.txt', sep='\t')
data1 = pd.read_csv('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/fold_1_data.txt', sep='\t')
data2 = pd.read_csv('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/fold_2_data.txt', sep='\t')
data3 = pd.read_csv('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/fold_3_data.txt', sep='\t')
data4 = pd.read_csv('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/fold_4_data.txt', sep='\t')

data.shape
total_shape = (data.shape[0]+data1.shape[0]+data2.shape[0]+data3.shape[0]+data4.shape[0], data.shape[1])
print(total_shape)

data.columns

data.dtypes

data.describe()

data.head(10)

data.fillna("0", inplace=True)
data1.fillna("0", inplace=True)
data2.fillna("0", inplace=True)
data3.fillna("0", inplace=True)
data4.fillna("0", inplace=True)

transform = transforms.Compose([
                                transforms.Resize((224, 224)),
                                transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                     std=[0.229, 0.224, 0.225])
])

all_pics_names = []
pics_path = []
for subdir, dirs, files1 in os.walk('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/faces'):
    for file in files1:
      if file.endswith(".jpg"):
        if file[26] == '.':
          f = file[27:]
        elif file[27] == '.':
          f = file[28:]
        elif file[28] == '.':
          f = file[29:]
        elif file[29] == '.':
          f = file[30:]
        all_pics_names.append(f)
        pics_path.append(os.path.join(subdir, file))

def findGender(pic):
  if data.index[data['original_image'] == pic].tolist():
    index = data.index[data['original_image'] == pic].tolist()[0]
    fold_file = data
  elif data1.index[data1['original_image'] == pic].tolist():
    index = data1.index[data1['original_image'] == pic].tolist()[0]
    fold_file = data1
  elif data2.index[data2['original_image'] == pic].tolist():
    index = data2.index[data2['original_image'] == pic].tolist()[0]
    fold_file = data2
  elif data3.index[data3['original_image'] == pic].tolist():
    index = data3.index[data3['original_image'] == pic].tolist()[0]
    fold_file = data3
  elif data4.index[data4['original_image'] == pic].tolist():
    index = data4.index[data4['original_image'] == pic].tolist()[0]
    fold_file = data4
  gender = fold_file['gender'][index]
  if gender == 'm':
    return "male"
  return "female"

class MyDataset(Dataset):
  def __init__(self, image_path, transform=None):
    super(MyDataset, self).__init__()
    self.transform = transform
    self.classes, self.class_to_idx = self._find_classes(image_path)
    self.samples = self.make_dataset(image_path, self.class_to_idx)
    self.targets = [s[1] for s in self.samples]

  def _find_classes(self, dir):
    classes = ['male', 'female']
    class_to_idx = {'male' : 0,
                    'female' : 1}
    return classes, class_to_idx

  def _get_target(self, file_path):
    if file_path[26] == '.':
      f = file_path[27:]
    if file_path[27] == '.':
      f = file_path[28:]
    elif file_path[28] == '.':
      f = file_path[29:]
    elif file_path[29] == '.':
      f = file_path[30:]
    return findGender(f)
  
  def make_dataset(self, dir, class_to_idx):
    images = []
    for subdir, dirs, files in os.walk('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/faces'):
      for file in files:
        if file.endswith(".jpg"):
          target = self._get_target(file)
          item = (os.path.join(subdir, file), class_to_idx[target])
          images.append(item)
    return images

  def __getitem__(self, index):
    path, target = self.samples[index]
    sample = self.default_loader(path)
    sample = self.transform(sample)
    return sample, target

  def get_class_dict(self):
    return self.class_to_idx

  def default_loader(self, path):
    if get_image_backend() == 'accimage':
      return self.accimage_loader(path)
    else:
      return self.pil_loader(path)

  def __len__(self):
    return len(self.samples)

  def accimage_loader(self, path):
    import accimage
    try:
      return accimage.Image(path)
    except IOError:
      return self.pil_loader(path)

  def pil_loader(self, path):
    with open(path, 'rb') as f:
      img = Image.open(f)
      return img.convert('RGB')

my_dataset = MyDataset('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/faces', transform)

print(my_dataset.classes)
print(my_dataset.class_to_idx)

classes = my_dataset.classes
print(len(classes))
print(classes)

train_data, test_data = torch.utils.data.random_split(my_dataset, [15496, 3874])

print('Train dataset size:', len(train_data))
print('Test dataset size:', len(test_data))

train_loader = DataLoader(train_data,
                          batch_size=16,
                          shuffle=True)

test_loader = DataLoader(test_data,
                          batch_size=16,
                          shuffle=False)

def show_imgs(imgs, title):
  imgs = imgs.cpu()
  mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)
  std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)

  imgs = imgs * std + mean
  img_grid = torchvision.utils.make_grid(imgs, nrow=4)
  img_np = img_grid.numpy()
  img_np = np.transpose(img_np, (1,2,0))

  plt.figure(figsize=(8,4))
  plt.imshow(img_np)
  plt.title(title)
  plt.show()

print('Training Examples: ')

for i, batch in enumerate(train_loader):
  if i == 4:
    break
  images, labels = batch
  show_imgs(images, str([classes[i] for i in labels]))

print('Test Examples: ')

for i, batch in enumerate(test_loader):
  if i == 4:
    break
  images, labels = batch
  show_imgs(images, str([classes[i] for i in labels]))

resnet = torchvision.models.resnet18(pretrained=True)
resnet

torchsummary.summary(resnet.to(device), input_size=(3,224,224))

for param in resnet.parameters():
  param.requires_grad = False

resnet.fc

in_features = resnet.fc.in_features
resnet.fc = nn.Linear(in_features, len(classes))

resnet.fc

for param in resnet.parameters():
  if param.requires_grad == True:
    print(param.size())

def evaluate(model, dataloader):
  total, correct = 0, 0
  model.eval()
  for batch in dataloader:
    images, labels = batch
    images, labels = images.to(device), labels.to(device)

    outs = model(images)
    outs_labels = torch.argmax(outs, axis=1)

    total += labels.size(0)
    correct += (labels == outs_labels).sum().item()
    
  return 100 * correct/total

import copy


def train(model, modelname, loss_fn, optimizer, train_loader, test_loader, epochs):
  hist = {'epoch_loss': [],
          'train_acc': [],
          'test_acc': []}
  min_loss = 10000
  for epoch in tqdm(range(1, epochs+1), total=epochs, desc='Training last layer'):
    losses = []
    for batch in train_loader:
      images, labels = batch
      images, labels = images.to(device), labels.to(device)

      model.train()

      outs = model(images)
      loss = loss_fn(outs, labels)

      losses.append(loss.item())

      optimizer.zero_grad()
      loss.backward()

      optimizer.step()

      del images, labels, outs
      torch.cuda.empty_cache()

    curr_epoch_loss = np.array(losses).mean()
    hist['epoch_loss'].append(curr_epoch_loss)
    hist['train_acc'].append(evaluate(model, train_loader))
    hist['test_acc'].append(evaluate(model, test_loader))

    if curr_epoch_loss < min_loss:
      min_loss = curr_epoch_loss
      best_model = copy.deepcopy(model.state_dict())

  fig, ax = plt.subplots(ncols=2, figsize=(12,6))
  ax[0].plot(range(1, epochs+1), hist['epoch_loss'], label='Loss')
  ax[0].plot(range(1, epochs+1), np.ones(epochs)*min_loss, 'r--', alpha=0.6,
             label='Min Loss={}'.format(min_loss))
  ax[0].set_xlabel('Epochs')
  ax[0].set_ylabel('Loss')
  ax[0].set_title('Epochs vs. Loss')
  ax[0].grid()
  ax[0].legend()

  ax[1].plot(range(1, epochs+1), hist['train_acc'], 'b--', alpha=0.8,
             label='Train Accuracy')
  ax[1].plot(range(1, epochs+1), hist['test_acc'], 'r--', alpha=0.8,
             label='Test Accuracy')
  ax[1].set_xlabel('Epochs')
  ax[1].set_ylabel('Accuracy Score')
  ax[1].set_title('Epochs vs. Accuracy Score')
  ax[1].grid()
  ax[1].legend()

  plt.plot()

  torch.save(best_model, '{0}_{1:.4f}.pth'.format(modelname, min_loss))
  print("Best loss value: {}".format(min_loss))

  files.download('{0}_{1:.4f}.pth'.format(modelname, min_loss))

  return best_model

resnet = resnet.to(device)

opt = torch.optim.Adam(resnet.parameters(), lr=0.0001)
loss_fn = nn.CrossEntropyLoss()

resnet_wts = train(resnet, 'resnet',
                   loss_fn, opt,
                   train_loader,
                   test_loader, 10)

resnet.load_state_dict(torch.load('/content/drive/My Drive/Classroom/ML Internship IndianServers Batch 14/resnet_0.5625(FP_gender)(1).pth'))

resnet = resnet.cpu()
test_batch1 = next(iter(test_loader))
pred_batch1 = resnet(test_batch1[0])
pred_labels1 = torch.argmax(pred_batch1, axis=1)
print("Predictions    :  ", end=" ")
for i in range(len(pred_labels1)):
  print(classes[pred_labels1[i].item()], end=", ")
print()
print("Actual Labels  :  ", end=" ")
for i in range(len(test_batch1[1])):
  print(classes[test_batch1[1][i].item()], end=", ")
print()
n = 0
for i in range(len(pred_batch1)):
  if (pred_labels1[i].item() == test_batch1[1][i].item()):
    n += 1
print("Accuracy : " + str(100*(n/len(pred_batch1))))

def findAge(pic):
  if data.index[data['original_image'] == pic].tolist():
    index = data.index[data['original_image'] == pic].tolist()[0]
    fold_file = data
  elif data1.index[data1['original_image'] == pic].tolist():
    index = data1.index[data1['original_image'] == pic].tolist()[0]
    fold_file = data1
  elif data2.index[data2['original_image'] == pic].tolist():
    index = data2.index[data2['original_image'] == pic].tolist()[0]
    fold_file = data2
  elif data3.index[data3['original_image'] == pic].tolist():
    index = data3.index[data3['original_image'] == pic].tolist()[0]
    fold_file = data3
  elif data4.index[data4['original_image'] == pic].tolist():
    index = data4.index[data4['original_image'] == pic].tolist()[0]
    fold_file = data4
  age = fold_file['age'][index]
  return age

class MyDatasetAge(Dataset):
  def __init__(self, image_path, transform=None):
    super(MyDatasetAge, self).__init__()
    self.transform = transform
    self.classes, self.class_to_idx = self._find_classes(image_path)
    self.samples = self.make_dataset(image_path, self.class_to_idx)
    self.targets = [s[1] for s in self.samples]

  def _find_classes(self, dir):
    classes = []
    for i in range(len(data['age'])):
      if (data['age'][i] not in classes):
        classes.append(data['age'][i])
    for i in range(len(data1['age'])):
      if (data1['age'][i] not in classes):
        classes.append(data1['age'][i])
    for i in range(len(data2['age'])):
      if (data2['age'][i] not in classes):
        classes.append(data2['age'][i])
    for i in range(len(data3['age'])):
      if (data3['age'][i] not in classes):
        classes.append(data3['age'][i])
    for i in range(len(data4['age'])):
      if (data4['age'][i] not in classes):
        classes.append(data4['age'][i])
    class_to_idx = {classes[i]: i for i in range(len(classes))}
    return classes, class_to_idx

  def _get_target(self, file_path):
    if file_path[26] == '.':
      f = file_path[27:]
    if file_path[27] == '.':
      f = file_path[28:]
    elif file_path[28] == '.':
      f = file_path[29:]
    elif file_path[29] == '.':
      f = file_path[30:]
    return findAge(f)
  
  def make_dataset(self, dir, class_to_idx):
    images = []
    for subdir, dirs, files in os.walk('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/faces'):
      for file in files:
        if file.endswith(".jpg"):
          target = self._get_target(file)
          item = (os.path.join(subdir, file), class_to_idx[target])
          images.append(item)
    return images

  def __getitem__(self, index):
    path, target = self.samples[index]
    sample = self.default_loader(path)
    sample = self.transform(sample)
    return sample, target

  def get_class_dict(self):
    return self.class_to_idx

  def default_loader(self, path):
    if get_image_backend() == 'accimage':
      return self.accimage_loader(path)
    else:
      return self.pil_loader(path)

  def __len__(self):
    return len(self.samples)

  def accimage_loader(self, path):
    import accimage
    try:
      return accimage.Image(path)
    except IOError:
      return self.pil_loader(path)

  def pil_loader(self, path):
    with open(path, 'rb') as f:
      img = Image.open(f)
      return img.convert('RGB')

my_dataset_age = MyDatasetAge('/content/drive/My Drive/AdienceBenchmarkGenderAndAgeClassification/faces', transform)

print(my_dataset_age.classes)
print(my_dataset_age.class_to_idx)

classes1 = my_dataset_age.classes
print(len(classes1))
print(classes1)

train_data1, test_data1 = torch.utils.data.random_split(my_dataset_age, [15496, 3874])

print('Train dataset size:', len(train_data1))
print('Test dataset size:', len(test_data1))

train_loader1 = DataLoader(train_data1,
                          batch_size=16,
                          shuffle=True)

test_loader1 = DataLoader(test_data1,
                          batch_size=16,
                          shuffle=False)

resnet_age = torchvision.models.resnet18(pretrained=True)
resnet_age

torchsummary.summary(resnet_age.to(device), input_size=(3,224,224))

for param in resnet_age.parameters():
  param.requires_grad = False

resnet_age.fc

in_features = resnet_age.fc.in_features
resnet_age.fc = nn.Linear(in_features, len(classes1))

resnet_age.fc

for param in resnet_age.parameters():
  if param.requires_grad == True:
    print(param.size())

resnet_age = resnet_age.to(device)

opt = torch.optim.Adam(resnet_age.parameters(), lr=0.0001)
loss_fn = nn.CrossEntropyLoss()

resnet_age_wts = train(resnet_age, 'resnet_age',
                      loss_fn, opt,
                      train_loader1,
                      test_loader1, 10)

resnet_age.load_state_dict(torch.load('/content/drive/My Drive/Classroom/ML Internship IndianServers Batch 14/resnet_age_1.7669(FP_age).pth'))

resnet_age = resnet_age.cpu()
test_batch2 = next(iter(test_loader1))
pred_batch2 = resnet_age(test_batch2[0])
pred_labels2 = torch.argmax(pred_batch2, axis=1)
print("Predictions    :  ", end=" ")
for i in range(len(pred_labels2)):
  print(classes1[pred_labels2[i].item()], end=", ")
print()
print("Actual Labels  :  ", end=" ")
for i in range(len(test_batch2[1])):
  print(classes1[test_batch2[1][i].item()], end=", ")
print()
n = 0
for i in range(len(pred_batch2)):
  if (pred_labels2[i].item() == test_batch2[1][i].item()):
    n += 1
print("Accuracy : " + str(100*(n/len(pred_batch2))))

test_image = '/content/drive/My Drive/Classroom/ML Internship IndianServers Batch 14/Datasets/test_image_9.jpg'

def demo_gender(path):
  img = Image.open(path)
  img = transform(img)
  img = img.unsqueeze(0)
  model = resnet.cpu()
  pred = model(img)
  pred = torch.argmax(pred, axis=1)
  print(classes[pred.item()])

def demo_age(path):
  img = Image.open(path)
  img = transform(img)
  img = img.unsqueeze(0)
  model = resnet_age.cpu()
  pred = model(img)
  pred = torch.argmax(pred, axis=1)
  print(classes1[pred.item()])

def demo_gender_age(pic):
  img = Image.open(pic)
  plt.imshow(img)
  demo_gender(pic)
  demo_age(pic)

demo_gender_age(test_image)